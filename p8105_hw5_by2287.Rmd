---
title: "Iterations"
author: "Binyam Yilma"
output: github_document
---


```{r, include = F}
library(tidyverse)
library(httr)
library(purrr)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1


Import + Tidy
```{r, message=FALSE}
homicide_df = 
  read_csv("./data/homicide-data.csv") %>% 
  mutate(
    city_state = str_c(city, state, sep = "_"),
    resolved = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved",
    )
  ) %>% 
  filter(city_state != "Tulsa_AL") 
```

The dataframe `homicide_df` contains information on homicides in 50 large U.S. cities. The dataset contains`r nrow(homicide_df)` rows and `r ncol(homicide_df)` columns, which contain information such as:`r homicide_df %>% select(reported_date, city, state, victim_race, victim_age, disposition) %>% names()`.

Notes on tidying - after importing, we concatenate the `city` & `state` variable into a combined `city_state` variable. We also create a new variable `resolved` that describes the status of the homicide case: solved or unsolved. We combine those cases with dispositions "Closed without arrest" or "Open/No arrest" as `unsolved` cases. Finally, we drop the city_state `Tulsa_AL` because it only contains one observation, which is most likely a data entry mistake.

Here we create a summary of the summary of the `homicide_df` showing the total number of homicides and the number of unsolved cases by `city_state`.
```{r, message=FALSE, warning = FALSE}
aggregate_df = homicide_df %>% 
  group_by(city_state) %>% 
  summarize(
   `total_homicide` = n(),
   `num_unsolved` = sum(resolved == "unsolved")
  ) 

aggregate_df %>% 
  head(5) %>% 
    knitr::kable()

```

Here we perform a proportion test on `Baltimore_MD`, to see if the proportion of unsolved cases is truly different than the proportion of solved cases. 
```{r}
p_test = prop.test(
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(num_unsolved), 
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(total_homicide)
  ) %>% 
  broom::tidy() %>% select(estimate, conf.low, conf.high)


p_test %>% 
  knitr::kable()
```

Here we perform a proportion test on all `city_state`'s to see if the proportion of unsolved cases is truly different than the proportion of solved cases. 
```{r}
prop_test = aggregate_df %>% 
  mutate(
    prop_tests = map2(.x = num_unsolved, .y = total_homicide, ~prop.test(x = .x, n = .y)),
    tidy_tests = map(.x = prop_tests, ~broom::tidy(.x))
  ) %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high)
```


Making a plot of the proption test estimates, with confidence intervals, by city. The proportion test here tests whether the proportion of unsolved homicide cases is equal to the proprtion of solved homicide cases. 
```{r, fig.width=8, fig.height=12}
prop_test %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 60, vjust = 1.0, hjust = 1)) + 
  labs(
    title = "Proportion Test Estimates, by City",
    x = "City",
    y = "Proportion Estimate"
  )
```

We observe here that `Chicago_IL` has the highest number of unsolved homicide cases, followed up `New_Orleans_LA`. By contrast, `Richmond_VA` has the lowest unsolved homicide rate, followed by `Charlotte_NC`.


## Problem 2
```{r, message=FALSE}
lda = tibble(
    path = list.files("./data/lda_data"),
  ) %>% 
  mutate(
    id = str_remove(path, ".csv"),
    path = str_c("./data/lda_data/", path),
    data = map(path, read_csv),
    data = map(data, bind_rows)
    ) %>% 
    select(-path) %>% 
    separate(id, into = c("treatment_arm", "id"), sep = "_") %>% 
  unnest(data) %>% 
    relocate(id) %>% 
  pivot_longer(
    cols = week_1:week_8,
    names_to = "week",
    names_prefix = "week_",
    values_to = "observation"
  ) %>% 
  mutate(
    week = as.numeric(week)
  )
    

lda %>% 
  head(5) %>% 
  knitr::kable()
```

The dataset `lda` is tidy data that contains data from a longitudinal study on `r nrow(lda)` participants, including the subject ID, arm, and observations over time. The tidying steps included first creating a tibble contains the `path` name of each file, which I then iterate over in the `mutate` step to read in those files. I then, create a dataframe that binds the rows of these read in data. I also create an `arm` and an `id` column off of the file names. Finally, I pivot the observations for each person by week into a long week to create a `week` and an `observation` variable; then I convert `week` into numeric to prepare it for the `spaghetti plot` shown below.


**Spaghetti Plot**
```{r fig.width=8, fig.height=12}
lda %>% 
  ggplot(aes(x = week, y = observation, group = interaction(treatment_arm, id), color = treatment_arm)) + 
  geom_line() + 
      labs(
    title = "Response to Treatment by arm",
    x = "Time in Weeks",
    y = "Observation"
  ) + 
  scale_color_discrete(name = "Treatment Arm",
                         breaks = c("con", "exp"),
                         labels = c("Control", "Experimental"))
```
We see that those who were on the treatment arm had, in general, a higher number of observations of the measured value, than those who were in the control arm. 


## Problem 3

First, I define a function `ttest` that outputs the results of a one-sample t-test, in specific the estimated mu_hat & p-value
from number generated from a random normal distrbution, with a fixed sample size `samp_size` of 30 and a standard deviation `sigma` of 5. 
```{r}
ttest = function(mu, samp_size = 30, sigma = 5) {
  sim_data = 
    tibble(
      x = rnorm(n = samp_size, mean = mu, sd = sigma)
    )
  
   sim_data %>% 
     summarize(
       mu_hat = t.test(x) %>% broom::tidy() %>% pull(estimate),
       p_value = t.test(x) %>% broom::tidy() %>% pull(p.value)
       
    )
}

```

Here, I create a dataframe that simulates the 5000 samples of size `30` and standard deviation of `5` form the random normal distribution. Then, I perform a one-sample ttest on these samples, and extract the estimate `mu_hat` and the `p_value` from each of these samples. I save these results in the dataframe `ttest_results`.
```{r cache=T}
ttest_results = tibble(
  mu = c(0,1,2,3,4,5,6)
) %>% 
  mutate(
    output_lists = map(.x = mu, ~rerun(5000, ttest(.x))),
    estimate_df = map(output_lists, bind_rows)
  ) %>% 
  select(-output_lists) %>% 
  unnest(estimate_df) 
```


##### Simulation Plots

Plot showing the proportion of times the null was rejected vs. the true value of the mean. 
```{r, message= FALSE}
ttest_results %>% 
  group_by(mu) %>% 
  summarize(
    total = n(), 
    num_rejected = sum(p_value < 0.05), 
    prop_rejected = num_rejected/total
  ) %>% 
  ggplot(aes(x = mu, y = prop_rejected)) + 
  geom_point() + 
  geom_smooth(se = F) + 
  labs(
    title = "Power Vs. Effect Size",
    x = "True Population Mean",
    y = "Power"
  )

```

In the plot above, we see that the power of a test has a sigmoidal relationship with the effect size, as effect size increases so does power, up to an extent, then it starts to plateau. 



An overlated Plot showing the average estimate we obtain from the ttest (mu_hat) vs. the true value of the mean. The red dashed line indicates the relationship of `mu_hat` with the true population `mu` for those tests whose null hypothesis was rejected at alpha = 0.05 i.e., p-value < 0.05. The solid Black line is for all tests (rejected Ho + not-rejected Ho).

```{r, message= FALSE, fig.width=8, fig.height=12}
#create a df that only filters on those tests whose H0 were rejected (p-value < 0.05)

ttest_results2 = ttest_results %>% 
  filter(p_value < 0.05) %>% 
  group_by(mu) %>% 
  summarize(
    mean_mu_hat = mean(mu_hat)
  ) 


#plot a line that shows the relationship between the average estimate mu_hat & the true mean mu overall, as well as for those tests with p-values less than 0.05
ttest_results %>% 
  group_by(mu) %>% 
  summarize(
    mean_mu_hat = mean(mu_hat)
  ) %>% 
  ggplot(aes(y = mean_mu_hat, x = mu)) + 
  geom_line() + 
  geom_point() +
    geom_line(data = ttest_results2, color = "red", linetype = "dashed") +
    geom_point(data = ttest_results2, color = "red") + 
  labs(
    title = "Mean of Estimate Vs. True Mean",
    x = "True Population Mean",
    y = "Mean of Estimate",
    caption = "Dashed, Red Line shows those tests with Rejected Null Ho \n
            Solid Black Line shows all tests (Rejected + Not Rejected)"
  ) 
```


In the plot above, we observe that the sample average mu_hat is only approximately equal to the true value of the `mu` for large `mu` (in this case for `mu` > 3) but it is markedly different for `mu` values that are approximately in between 1 - 2. I believe we observe this due to the difference in the effect size. For large effect sizes, the power of a test increases, however, as the effect size decreases, the power of a test decreases. In this simulation, 

