---
title: "Iterations"
author: "Binyam Yilma"
date: "11/16/2020"
output: github_document
---


```{r, include = F}
library(tidyverse)
library(httr)
library(purrr)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1


Import + Tidy
```{r, message=FALSE}
homicide_df = 
  read_csv("./data/homicide-data.csv") %>% 
  mutate(
    city_state = str_c(city, state, sep = "_"),
    resolved = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved",
    )
  ) %>% 
  filter(city_state != "Tulsa_AL") #filter out Tulsa_AL because it only contains one observation - probably a data entry mistake
```

The dataframe `homicide_df` contains information on homicides in 50 large U.S. cities. The dataset contains`r nrow(homicide_df)` rows and `r ncol(homicide_df)` columns, which contain information such as:`r homicide_df %>% select(reported_date, city, state, victim_race, victim_age, disposition) %>% names()`.

#  comment on disposition

```{r, message=FALSE, warning = FALSE}
aggregate_df = homicide_df %>% 
  group_by(city_state) %>% 
  summarize(
   `total_homicide` = n(),
   `num_unsolved` = sum(resolved == "unsolved")
  ) 

aggregate_df %>% 
  head(5) %>% 
    knitr::kable()

```


```{r}
p_test = prop.test(
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(num_unsolved), 
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(total_homicide)
  ) %>% 
  broom::tidy() %>% select(estimate, conf.low, conf.high)


p_test %>% 
  knitr::kable()
```


```{r}
prop_test = aggregate_df %>% 
  mutate(
    prop_tests = map2(.x = num_unsolved, .y = total_homicide, ~prop.test(x = .x, n = .y)),
    tidy_tests = map(.x = prop_tests, ~broom::tidy(.x))
  ) %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high)
```


Making a plot of the proption test estimates, with confidence intervals, by city. The proportion test here tests whether the proportion of unsolved homicide cases is equal to the proprtion of solved homicide cases. 

```{r, fig.width=8, fig.height=12}
prop_test %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 60, vjust = 1.0, hjust = 1)) + 
  labs(
    title = "Proportion Test Estimates, by City",
    x = "City",
    y = "Proportion Estimate"
  )
```

We notice that the `Chicago_IL` has the highest number of unsolved homicide cases, followed up `New_Orleans_LA`. By contrast, `Richmond_VA` has the lowest unsolved homicide rate, followed by `Charlotte_NC`.


## Problem 2

```{r, message=FALSE}
lda = tibble(
    path = list.files("./data/lda_data"),
  ) %>% 
  mutate(
    id = str_remove(path, ".csv"),
    path = str_c("./data/lda_data/", path),
    data = map(path, read_csv),
    data = map(data, bind_rows)
    ) %>% 
    select(-path) %>% 
    separate(id, into = c("treatment_arm", "id"), sep = "_") %>% 
  unnest(data) %>% 
    relocate(id) %>% 
  pivot_longer(
    cols = week_1:week_8,
    names_to = "week",
    names_prefix = "week_",
    values_to = "observation"
  ) %>% 
  mutate(
    week = as.numeric(week)
  )
    

lda %>% 
  head(5) %>% 
  knitr::kable()

```


The dataset `lda` is tidy data that contains data from a longitudinal study on `r nrow(lda)` participants, including the subject ID, arm, and observations over time. The tidying steps included first creating a tibble contains the `path` name of each file, which I then iterate over in the `mutate` step to read in those files. I then, create a dataframe that binds the rows of these read in data. I also create an `arm` and an `id` column off of the file names. Finally, I pivot the observations for each person by week into a long week to create a `week` and an `observation` variable; then I convert `week` into numeric to prepare it for the `spaghetti plot` shown below.



Spaghetti plot
```{r}
lda %>% 
  ggplot(aes(x = week, y = observation, group = interaction(treatment_arm, id), color = treatment_arm)) + 
  geom_line() + 
      labs(
    title = "Response to treatment by arm",
    x = "Time in Weeks",
    y = "Observation"
  ) + 
  scale_color_discrete(name = "Treatment Arm",
                         breaks = c("con", "exp"),
                         labels = c("Control", "Experimental"))
```

We see that those who were on the treatment arm had, in general, a higher number of observations of the measured value, than those who were in the control arm. 


## Problem 3

First, I define a function `ttest` that outputs the results of a one-sample t-test, in specific the estimated mu_hat & p-value
from number generated from a random normal distrbution, with a fixed sample size `samp_size` of 30 and a standard deviation `sigma` of 5. 
```{r}
ttest = function(mu, samp_size = 30, sigma = 5) {
  sim_data = 
    tibble(
      x = rnorm(n = samp_size, mean = mu, sd = sigma)
    )
  
   sim_data %>% 
     summarize(
       mu_hat = t.test(x) %>% broom::tidy() %>% pull(estimate),
       p_value = t.test(x) %>% broom::tidy() %>% pull(p.value)
       
    )
}

```

Here, I create a dataframe that simulates the 5000 samples of size `30` and standard deviation of `5` form the random normal distribution. Then, I perform a one-sample ttest on these samples, and extract the estimate `mu_hat` and the `p_value` from each of these samples. I save these results in the dataframe `ttest_results`.
```{r cache=T}
ttest_results = tibble(
  mu = c(0,1,2,3,4,5,6)
) %>% 
  mutate(
    output_lists = map(.x = mu, ~rerun(5000, ttest(.x))),
    estimate_df = map(output_lists, bind_rows)
  ) %>% 
  select(-output_lists) %>% 
  unnest(estimate_df) 
```

#### Plots

In the plot below, we see that the power of a test has a sigmoidal relationship with the effect size, as effect size increases so does power, up to an extent, then it starts to plateau. 
```{r, message= FALSE}
ttest_results %>% 
  group_by(mu) %>% 
  summarize(
    total = n(), 
    num_rejected = sum(p_value < 0.05), 
    prop_rejected = num_rejected/total
  ) %>% 
  ggplot(aes(x = mu, y = prop_rejected)) + 
  geom_point() + 
  geom_smooth(se = F) + 
  labs(
    title = "Power Vs. Effect Size",
    x = "True Population Mean",
    y = "Power"
  )

```


```{r, message= FALSE}
#create a df that only filters on those tests whose H0 were rejected (p-value < 0.05)

ttest_results2 = ttest_results %>% 
  filter(p_value < 0.05) %>% 
  group_by(mu) %>% 
  summarize(
    mean_mu_hat = mean(mu_hat)
  ) 


#plot a line that shows the relationship between the average estimate mu_hat & the true mean mu overall, as well as for those tests with p-values less than 0.05
ttest_results %>% 
  group_by(mu) %>% 
  summarize(
    mean_mu_hat = mean(mu_hat)
  ) %>% 
  ggplot(aes(y = mean_mu_hat, x = mu)) + 
  geom_line() + 
  geom_point() +
    geom_line(data = ttest_results2, color = "red", linetype = "dashed") +
    geom_point(data = ttest_results2, color = "red") + 
  labs(
    title = "Mean of Estimate Vs. True Mean",
    x = "True Population Mean",
    y = "Mean of Estimate",
    caption = "Dashed, Red Line shows those tests with Rejected Null Ho \n
            Solid Black Line shows all tests (Rejected + Not Rejected)"
  ) 
```


